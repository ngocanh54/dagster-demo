# Dagster Demo Project

A demo project showing Dagster's factory pattern (YAML-defined pipelines) for users coming from Apache Airflow.

## ğŸš€ Quick Start

```bash
# Install dependencies
make install

# Start Dagster with both code locations
make dev

# Access UI at http://localhost:3000
```

## ğŸ“‹ Prerequisites

- Python 3.8+
- pip

## ğŸ—ï¸ Project Structure

```
dagster_demo/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion_pipelines/         # Code Location 1: Raw data ingestion (YAML)
â”‚   â”‚   â”œâ”€â”€ definitions.py          # Loads YAML pipelines
â”‚   â”‚   â””â”€â”€ config/                 # YAML pipeline configs (with schedules)
â”‚   â”‚       â”œâ”€â”€ sample_pipeline.yaml
â”‚   â”‚       â””â”€â”€ albums_pipeline.yaml
â”‚   â”‚
â”‚   â”œâ”€â”€ data_marts/                 # Code Location 2: Analytics marts (Python)
â”‚   â”‚   â”œâ”€â”€ definitions.py          # Loads Python assets
â”‚   â”‚   â””â”€â”€ assets.py               # Mart transformations
â”‚   â”‚
â”‚   â””â”€â”€ shared/                     # Shared utilities
â”‚       â””â”€â”€ factories/              # Factory pattern (like Airflow's plugins/)
â”‚           â”œâ”€â”€ __init__.py
â”‚           â””â”€â”€ asset_builder.py   # YAML â†’ Assets + Schedules
â”‚
â”œâ”€â”€ workspace.yaml                  # Defines both code locations
â”œâ”€â”€ Makefile                        # Common commands
â”œâ”€â”€ pyproject.toml                  # Dependencies
â”œâ”€â”€ README.md                       # This file
â””â”€â”€ AIRFLOW_COMPARISON.md           # Detailed Airflow vs Dagster comparison
```

## ğŸ¯ What This Demo Shows

### 1. Factory Pattern (YAML-Defined Pipelines)

Similar to Airflow's `plugins/dagbuilder.py` pattern where entire pipelines are defined in YAML.

**Airflow approach:**
```yaml
# config/dag.yaml
dag_id: my_pipeline
schedule: "0 2 * * *"
tasks:
  fetch_data:
    operator: HttpOperator
```

**Dagster approach (this project):**
```yaml
# config/pipeline.yaml
assets:
  fetch_data:
    type: api_fetch
    url: https://api.example.com/data

schedules:
  - name: daily_refresh
    cron: "0 2 * * *"
    asset_selection: "*"
```

### 2. Multiple Code Locations

Like having separate DAG folders in Airflow:
- **`ingestion_pipelines`**: Raw data ingestion (YAML-defined)
- **`data_marts`**: Analytics transformations (Python)

### 3. Cross-Code-Location Dependencies

Data marts depend on ingestion pipelines:

```
ingestion_pipelines          data_marts
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ raw_todos   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ user_activity_ â”‚
â”‚ raw_commentsâ”‚            â”‚ mart           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Available Commands

| Command | Description |
|---------|-------------|
| `make install` | Install dependencies in virtual environment |
| `make dev` | Start Dagster webserver with both code locations |
| `make test` | Run test suite |
| `make clean` | Remove build artifacts |
| `make help` | Show all available commands |

## ğŸ“– Understanding the Factory Pattern

### Creating a YAML Pipeline

**Step 1:** Create a YAML config file

```yaml
# src/ingestion_pipelines/config/my_pipeline.yaml
assets:
  raw_data:
    type: api_fetch
    url: https://api.example.com/data
    description: Fetch data from API

  processed_data:
    type: transform
    description: Process the raw data
    depends_on:
      - raw_data

schedules:
  - name: daily_pipeline
    cron: "0 2 * * *"
    asset_selection: "*"
```

**Step 2:** Load it in definitions.py

```python
# src/ingestion_pipelines/definitions.py
from shared.factories import build_from_yaml
import os

pipeline = build_from_yaml("config/my_pipeline.yaml")

defs = Definitions(
    assets=pipeline['assets'],
    schedules=pipeline['schedules'],
    jobs=pipeline['jobs'],
)
```

**Step 3:** Run it

```bash
make dev
# Go to http://localhost:3000
# Click "Materialize all"
```

### Supported Asset Types

#### `api_fetch`
Fetches data from HTTP endpoint, saves as CSV.

```yaml
asset_name:
  type: api_fetch
  url: https://api.example.com/endpoint
  description: What this fetches
```

#### `transform`
Transforms or combines upstream data.

```yaml
asset_name:
  type: transform
  description: What this does
  depends_on:
    - upstream_asset_1
    - upstream_asset_2
```

### Schedules in YAML

Define schedules just like Airflow's `schedule_interval`:

```yaml
schedules:
  - name: daily_refresh
    cron: "0 2 * * *"          # Cron expression
    asset_selection: "*"       # "*" = all assets, or list specific ones
```

## ğŸ”— Data Flow in This Demo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ingestion_pipelines (YAML-defined)                           â”‚
â”‚                                                              â”‚
â”‚  raw_todos â”€â”€â”                                               â”‚
â”‚              â”œâ”€â”€> merged_data â”€â”€> final_analytics            â”‚
â”‚  raw_commentsâ”˜                                               â”‚
â”‚                                                              â”‚
â”‚  raw_albums â”€â”€â”                                              â”‚
â”‚               â”œâ”€â”€> enriched_albums â”€â”€> album_stats           â”‚
â”‚  raw_photos â”€â”€â”˜                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚ (dependencies)
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ data_marts (Python)                                          â”‚
â”‚                                                              â”‚
â”‚  user_activity_mart (uses raw_todos, raw_comments)           â”‚
â”‚  album_engagement_mart (uses raw_albums, raw_photos)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Output Files

All assets save data to `output/`:

```
output/
â”œâ”€â”€ raw_todos.csv
â”œâ”€â”€ raw_comments.csv
â”œâ”€â”€ user_activity_mart.csv
â””â”€â”€ ... (more CSV files)
```

## ğŸ†š Dagster vs Airflow

For detailed comparison, see **[AIRFLOW_COMPARISON.md](AIRFLOW_COMPARISON.md)**.

### Quick Comparison

| Airflow | Dagster (this project) |
|---------|------------------------|
| `dags/pipeline1/` | `ingestion_pipelines/` code location |
| `dags/pipeline2/` | `data_marts/` code location |
| `plugins/dagbuilder.py` | `shared/factories/asset_builder.py` |
| DAG config YAML | Pipeline config YAML |
| `schedule_interval: "0 2 * * *"` | `schedules: [{cron: "0 2 * * *"}]` |
| XCom for data passing | Direct function parameters |
| Task-centric | Asset-centric (data products) |

### Key Differences

**1. Data Passing**
- **Airflow**: XCom (push/pull, untyped)
- **Dagster**: Function parameters (type-safe)

**2. Dependencies**
- **Airflow**: `>>` operator + XCom
- **Dagster**: Function parameters define both

**3. Testing**
- **Airflow**: Complex mocking (XCom, context)
- **Dagster**: Test like regular Python functions

**4. Local Development**
- **Airflow**: Requires Docker
- **Dagster**: Just `make dev`

## ğŸ“ When to Use Each Pattern

### Use YAML (Factory Pattern) When:
âœ… Standard ingestion patterns (API â†’ CSV)
âœ… Many similar pipelines
âœ… Non-developers need to create pipelines
âœ… Consistency is important

### Use Python (Hand-Written) When:
âœ… Complex transformations (like `data_marts`)
âœ… Custom business logic
âœ… Type safety is priority
âœ… IDE autocomplete needed

### Use Multiple Code Locations When:
âœ… Different teams own different pipelines
âœ… Different deployment schedules
âœ… Logical separation (ingestion vs analytics)

## ğŸ› Troubleshooting

**Port already in use?**
```bash
lsof -ti:3000 | xargs kill -9
```

**Module not found?**
```bash
make clean
make install
```

**Code location failed to load?**
- Check for Python syntax errors in definitions.py
- Ensure YAML files are valid
- Check logs in terminal

## ğŸ”— Useful Resources

- [Dagster Docs](https://docs.dagster.io)
- [Dagster University](https://dagster.io/university) - Free courses
- [Airflow to Dagster Migration Guide](https://docs.dagster.io/integrations/airflow)
- [AIRFLOW_COMPARISON.md](AIRFLOW_COMPARISON.md) - Detailed comparison

## ğŸ“ Next Steps

1. âœ… Run `make dev` and explore the UI
2. âœ… View the asset graph to see dependencies across code locations
3. âœ… Materialize all assets to see the data flow
4. âœ… Check the Schedules tab for YAML-defined schedules
5. âœ… Create your own YAML pipeline in `config/`
6. âœ… Add custom transformations in `data_marts/defs/`

---

**Happy Orchestrating! ğŸ‰**
